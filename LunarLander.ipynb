{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(tianshou.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4RNGiXy9f4f",
        "outputId": "4f926d27-f9bc-412c-9fbd-41d8e83e57e7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Open AI Gym LunarLanderContinuous-v2\n",
        "Nick Kaparinos\n",
        "2021\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from tqdm import tqdm\n",
        "from stable_baselines3 import SAC\n",
        "from os import listdir\n",
        "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
        "import tianshou\n",
        "from tianshou import offpolicy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "eP28NepH9OS3",
        "outputId": "41cc6a04-9d99-4db1-862e-ee8704a439c3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'offpolicy' from 'tianshou' (/usr/local/lib/python3.11/dist-packages/tianshou/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52-1207648404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtianshou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtianshou\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moffpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'offpolicy' from 'tianshou' (/usr/local/lib/python3.11/dist-packages/tianshou/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class LogStepsCallback(BaseCallback):\n",
        "    def __init__(self, log_dir, verbose=0):\n",
        "        self.log_dir = log_dir\n",
        "        super(LogStepsCallback, self).__init__(verbose)\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        self.results = pd.DataFrame(columns=['Reward', 'Done'])\n",
        "        print(\"Τraining starts!\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if 'reward' in self.locals:\n",
        "            keys = ['reward', 'done']\n",
        "        else:\n",
        "            keys = ['rewards', 'dones']\n",
        "        self.results.loc[len(self.results)] = [self.locals[keys[0]][0], self.locals[keys[1]][0]]\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        self.results.to_csv(self.log_dir + 'training_data.csv', index=False)\n",
        "        print(\"Τraining ends!\")\n",
        "\n",
        "\n",
        "class TqdmCallback(BaseCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.progress_bar = None\n",
        "\n",
        "    def _on_training_start(self):\n",
        "        self.progress_bar = tqdm(total=self.locals['total_timesteps'])\n",
        "\n",
        "    def _on_step(self):\n",
        "        self.progress_bar.update(1)\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        self.progress_bar.close()\n",
        "        self.progress_bar = None\n",
        "\n",
        "\n",
        "def save_dict_to_file(dict, path, txt_name='hyperparameter_dict'):\n",
        "    f = open(path + '/' + txt_name + '.txt', 'w')\n",
        "    f.write(str(dict))\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def calc_episode_rewards(training_data):\n",
        "    # Calculate the rewards for each training episode\n",
        "    episode_rewards = []\n",
        "    temp_reward_sum = 0\n",
        "\n",
        "    for step in range(training_data.shape[0]):\n",
        "        reward, done = training_data.iloc[step, :]\n",
        "        temp_reward_sum += reward\n",
        "        if done:\n",
        "            episode_rewards.append(temp_reward_sum)\n",
        "            temp_reward_sum = 0\n",
        "\n",
        "    result = pd.DataFrame(columns=['Reward'])\n",
        "    result['Reward'] = episode_rewards\n",
        "    return result\n",
        "\n",
        "\n",
        "def learning_curve(episode_rewards, log_dir, window=10):\n",
        "    # Calculate rolling window metrics\n",
        "    rolling_average = episode_rewards.rolling(window=window, min_periods=window).mean().dropna()\n",
        "    rolling_max = episode_rewards.rolling(window=window, min_periods=window).max().dropna()\n",
        "    rolling_min = episode_rewards.rolling(window=window, min_periods=window).min().dropna()\n",
        "\n",
        "    # Change column name\n",
        "    rolling_average.columns = ['Average Reward']\n",
        "    rolling_max.columns = ['Max Reward']\n",
        "    rolling_min.columns = ['Min Reward']\n",
        "    rolling_data = pd.concat([rolling_average, rolling_max, rolling_min], axis=1)\n",
        "\n",
        "    # Plot\n",
        "    sns.set()\n",
        "    plt.figure(0)\n",
        "    ax = sns.lineplot(data=rolling_data)\n",
        "    ax.fill_between(rolling_average.index, rolling_min.iloc[:, 0], rolling_max.iloc[:, 0], alpha=0.2)\n",
        "    ax.set_title('Learning Curve')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.set_xlabel('Episodes')\n",
        "\n",
        "    # Save figure\n",
        "    plt.savefig(log_dir + 'learning_curve' + str(window) + '.png')\n",
        "\n",
        "\n",
        "def learning_curve_baselines(log_dir, window=10):\n",
        "    # Read data\n",
        "    training_data = pd.read_csv(log_dir + 'training_data.csv', index_col=None)\n",
        "\n",
        "    # Calculate episode rewards\n",
        "    episode_rewards = calc_episode_rewards(training_data)\n",
        "\n",
        "    learning_curve(episode_rewards=episode_rewards, log_dir=log_dir, window=window)\n",
        "\n",
        "\n",
        "def learning_curve_tianshou(log_dir, window=10):\n",
        "    # Find event file\n",
        "    files = listdir(log_dir)\n",
        "    for f in files:\n",
        "        if 'events' in f:\n",
        "            event_file = f\n",
        "            break\n",
        "\n",
        "    # Read episode rewards\n",
        "    episode_rewards_list = []\n",
        "    episode_rewards = pd.DataFrame(columns=['Reward'])\n",
        "    try:\n",
        "        for e in summary_iterator(log_dir + event_file):\n",
        "            if len(e.summary.value) > 0:\n",
        "                if e.summary.value[0].tag == 'train/reward':\n",
        "                    episode_rewards_list.append(e.summary.value[0].simple_value)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    episode_rewards['Reward'] = episode_rewards_list\n",
        "\n",
        "    # Learning curve\n",
        "    learning_curve(episode_rewards, log_dir, window=window)\n",
        "\n",
        "\n",
        "def learning_curve_tianshou_multiple_runs(log_dirs, window=10):\n",
        "    episode_rewards_list = []\n",
        "    episode_rewards = pd.DataFrame(columns=['Reward'])\n",
        "\n",
        "    for log_dir in log_dirs:\n",
        "        # Find event file\n",
        "        files = listdir(log_dir)\n",
        "        for f in files:\n",
        "            if 'events' in f:\n",
        "                event_file = f\n",
        "                break\n",
        "\n",
        "        # Read episode rewards\n",
        "\n",
        "        try:\n",
        "            for e in summary_iterator(log_dir + event_file):\n",
        "                if len(e.summary.value) > 0:\n",
        "                    if e.summary.value[0].tag == 'train/reward':\n",
        "                        episode_rewards_list.append(e.summary.value[0].simple_value)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    episode_rewards['Reward'] = episode_rewards_list\n",
        "\n",
        "    # Learning curve\n",
        "    learning_curve(episode_rewards, log_dir, window=window)"
      ],
      "metadata": {
        "id": "ICXoQkre5qXb"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OtU003244M96"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from gym import wrappers\n",
        "import gym\n",
        "import tianshou as ts\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tianshou.utils import TensorboardLogger\n",
        "from tianshou.exploration import GaussianNoise, OUNoise\n",
        "from tianshou.policy import SACPolicy\n",
        "import torch\n",
        "from tianshou.utils.net.common import Net\n",
        "from tianshou.utils.net.continuous import Critic, ActorProb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Box2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRRenELi5eMe",
        "outputId": "8a956021-0e70-49ff-ac0a-a62553813e14"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Box2D in /usr/local/lib/python3.11/dist-packages (2.3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    start = time.perf_counter()\n",
        "    env_id = \"LunarLanderContinuous-v2\"\n",
        "    seed = 0\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    load_pretrained_model = False\n",
        "    model_path = ''"
      ],
      "metadata": {
        "id": "vj8-KZaG65UL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    start = time.perf_counter()\n",
        "    env_id = \"LunarLanderContinuous-v2\"\n",
        "    seed = 0\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    load_pretrained_model = False\n",
        "    model_path = ''"
      ],
      "metadata": {
        "id": "H6zTcLCv47JU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    model_name = 'Tianshou_SAC'\n",
        "    log_dir = 'logs/' + model_name + '_' + str(time.strftime('%d_%b_%Y_%H_%M_%S', time.localtime())) + '/'\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    logger = TensorboardLogger(writer, train_interval=1, update_interval=1)\n",
        "\n",
        "    # Environment\n",
        "    env = gym.make(env_id)\n",
        "    env.seed(seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIQ06YnQ5EmI",
        "outputId": "77c505eb-75d9-41c9-f6af-1f3b69850f6a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_id)\n",
        "env.seed(seed=seed)\n",
        "\n",
        "train_envs = ts.env.DummyVectorEnv([lambda: gym.make(env_id) for _ in range(1)])\n",
        "test_envs = ts.env.DummyVectorEnv([lambda: gym.make(env_id) for _ in range(1)])\n",
        "train_envs.seed(seed)\n",
        "test_envs.seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlIdOlh6j8m",
        "outputId": "2d865fc9-8d9d-437f-bb3f-60a146ee01c7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0]]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Neural networks and policy\n",
        "    state_shape = env.observation_space.shape or env.observation_space.n\n",
        "    action_shape = env.action_space.shape or env.action_space.n\n",
        "    max_action = env.action_space.high[0]\n",
        "    model_hyperparameters = {'hidden_sizes': [256, 256], 'learning_rate': 1e-3, 'estimation_step': 1}"
      ],
      "metadata": {
        "id": "aaNcYhbW6qB8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Actor\n",
        "    net_a = Net(state_shape, hidden_sizes=model_hyperparameters['hidden_sizes'], device=device)\n",
        "    actor = ActorProb(net_a, action_shape, max_action=max_action, device=device, unbounded=True).to(device)\n",
        "    actor_optim = torch.optim.Adam(actor.parameters(), lr=model_hyperparameters['learning_rate'])"
      ],
      "metadata": {
        "id": "NO2je2106sZa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Critics\n",
        "    net_c1 = Net(state_shape, action_shape, hidden_sizes=model_hyperparameters['hidden_sizes'], concat=True,\n",
        "                 device=device)\n",
        "    critic1 = Critic(net_c1, device=device).to(device)\n",
        "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=model_hyperparameters['learning_rate'])\n",
        "    net_c2 = Net(state_shape, action_shape, hidden_sizes=model_hyperparameters['hidden_sizes'], concat=True,\n",
        "                 device=device)\n",
        "    critic2 = Critic(net_c2, device=device).to(device)\n",
        "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=model_hyperparameters['learning_rate'])"
      ],
      "metadata": {
        "id": "W-_taK5x6ugI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Alpha\n",
        "    target_entropy = -np.prod(env.action_space.shape)\n",
        "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
        "    alpha_lr = 1e-5\n",
        "    alpha_optim = torch.optim.Adam([log_alpha], lr=alpha_lr)\n",
        "    alpha = (target_entropy, log_alpha, alpha_optim)\n",
        "\n",
        "\n",
        "    policy = SAC(\"MlpPolicy\", env, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lVNG2-h6wYQ",
        "outputId": "1311e6b2-f3d5-4d13-edaf-752ed6ae6e93"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
            "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collectors\n",
        "use_prioritised_replay_buffer = False\n",
        "prioritized_buffer_hyperparameters = {'total_size': 1_000_000, 'buffer_num': 1, 'alpha': 0.4, 'beta': 0.5}\n",
        "if use_prioritised_replay_buffer:\n",
        "    train_collector = ts.data.Collector(policy, train_envs,\n",
        "                                        ts.data.PrioritizedVectorReplayBuffer(**prioritized_buffer_hyperparameters),\n",
        "                                        exploration_noise=True)\n",
        "else:\n",
        "    train_collector = ts.data.Collector(policy, train_envs,\n",
        "                                        ts.data.ReplayBuffer(size=prioritized_buffer_hyperparameters['total_size']),\n",
        "                                        exploration_noise=True)\n",
        "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "WmjWJvoY8AB1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sigma schedule\n",
        "def build_sigma_schedule(max_sigma=0.5, min_sigma=0.0, steps_per_epoch=50_000, decay_time_steps=10_000):\n",
        "    decay_per_step = (max_sigma - min_sigma) / decay_time_steps\n",
        "\n",
        "    def custom_sigma_schedule(epoch, env_step):\n",
        "        step_number = (epoch - 1) * steps_per_epoch + env_step\n",
        "\n",
        "        current_sigma = max_sigma - step_number * decay_per_step\n",
        "        if current_sigma < 0.0:\n",
        "            current_sigma = 0.0\n",
        "        policy._noise = GaussianNoise(sigma=current_sigma * max_action)\n",
        "\n",
        "    return custom_sigma_schedule"
      ],
      "metadata": {
        "id": "ljqjv1df8uvf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test function\n",
        "def build_test_fn(num_episodes):\n",
        "    def custom_test_fn(epoch, env_step):\n",
        "        print(f\"Epoch = {epoch}\")\n",
        "\n",
        "        # Save agent\n",
        "        torch.save(policy.state_dict(), log_dir + model_name + f'_epoch{epoch}.pth')\n",
        "\n",
        "        # Record agents performance in video\n",
        "        for episode in range(num_episodes):\n",
        "            env = ts.env.DummyVectorEnv(\n",
        "                [lambda: wrappers.Monitor(env=gym.make(env_id), directory=log_dir + '/videos/epoch_' + str(\n",
        "                    epoch) + '/video' + str(episode), force=False) for _ in range(1)])\n",
        "\n",
        "            # Video\n",
        "            policy.eval()\n",
        "            collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
        "            collector.collect(n_episode=1, render=1 / 60)\n",
        "\n",
        "    return custom_test_fn"
      ],
      "metadata": {
        "id": "qTSM_us28zWl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "trainer_hyperparameters = {'max_epoch': 4, 'step_per_epoch': 100_000, 'step_per_collect': 10,\n",
        "                            'episode_per_test': 10,\n",
        "                            'batch_size': 128}\n",
        "decay_steps = int(trainer_hyperparameters['max_epoch'] * trainer_hyperparameters['step_per_epoch'] * 0.05)\n",
        "build_sigma_hyperparameters = {'max_sigma': 1, 'min_sigma': 0.0, 'decay_time_steps': decay_steps}\n",
        "all_hypeparameters = dict(model_hyperparameters, **trainer_hyperparameters, **prioritized_buffer_hyperparameters)\n",
        "all_hypeparameters['seed'] = seed\n",
        "all_hypeparameters['use_prioritised_replay_buffer'] = use_prioritised_replay_buffer\n",
        "all_hypeparameters['alpha_lr'] = alpha_lr\n",
        "if load_pretrained_model:\n",
        "    policy.load_state_dict(torch.load(model_path))\n",
        "    all_hypeparameters['load_pretrained_model'] = load_pretrained_model\n",
        "    all_hypeparameters['model_path'] = model_path\n",
        "save_dict_to_file(all_hypeparameters, path=log_dir)\n",
        "\n",
        "result = ts.trainer.OffpolicyTrainer(policy, train_collector, test_collector, **trainer_hyperparameters,\n",
        "                                      train_fn=build_sigma_schedule(**build_sigma_hyperparameters,\n",
        "                                                                    steps_per_epoch=trainer_hyperparameters[\n",
        "                                                                        'step_per_epoch']),\n",
        "                                      test_fn=build_test_fn(num_episodes=4), stop_fn=None, logger=logger)\n",
        "print(f'Finished training! Use {result[\"duration\"]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4_-DyzAU83L4",
        "outputId": "ab459485-6d77-45a0-a316-da55e27fb4a4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaseTrainer.__init__() got multiple values for argument 'max_epoch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-58-916539925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msave_dict_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hypeparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m result = ts.trainer.OffpolicyTrainer(policy, train_collector, test_collector, **trainer_hyperparameters,\n\u001b[0m\u001b[1;32m     18\u001b[0m                                       train_fn=build_sigma_schedule(**build_sigma_hyperparameters,\n\u001b[1;32m     19\u001b[0m                                                                     steps_per_epoch=trainer_hyperparameters[\n",
            "\u001b[0;31mTypeError\u001b[0m: BaseTrainer.__init__() got multiple values for argument 'max_epoch'"
          ]
        }
      ]
    }
  ]
}